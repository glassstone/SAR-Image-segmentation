{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SegNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgPWAqcR2Lh8",
        "colab_type": "code",
        "outputId": "cc34406d-93e6-4537-ba31-2c68a1e5530a",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "#@title   importing drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUn3rtyp12d-",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "c4a595f1-61ff-4b29-be73-fb4cefb756a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#@title   generator.py\n",
        "#generator.py\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "\n",
        "def category_label(labels, dims, n_labels):\n",
        "    x = np.zeros([dims[0], dims[1], n_labels])\n",
        "    #print(labels)\n",
        "    for i in range(dims[0]):\n",
        "        for j in range(dims[1]):\n",
        "          x[i, j, (int)(labels[i][j]/255)] = 1\n",
        "    x = x.reshape(dims[0] * dims[1], n_labels)\n",
        "    return x\n",
        "\n",
        "\n",
        "def data_gen_small(img_dir, mask_dir, lists, batch_size, dims, n_labels):\n",
        "    while True:\n",
        "        ix = np.random.choice(np.arange(len(lists)), batch_size)\n",
        "        imgs = []\n",
        "        labels = []\n",
        "        for i in ix:\n",
        "            # images\n",
        "            img_path = img_dir + lists.iloc[i, 0] + '.tif'\n",
        "#            print( img_path )\n",
        "            original_img = cv2.imread(img_path)[:, :, ::-1]\n",
        "            resized_img = cv2.resize(original_img, (dims[0], dims[1]))\n",
        "            array_img = img_to_array(resized_img)\n",
        "#            print(array_img.size)\n",
        "            imgs.append(array_img)\n",
        "            # masks\n",
        "            original_mask = cv2.imread(mask_dir + lists.iloc[i, 0] + '.tif')\n",
        "            resized_mask = cv2.resize(original_mask, (dims[0], dims[1]))\n",
        "#            print(resized_mask.shape)\n",
        "            array_mask = category_label(resized_mask[:, :, 0], dims, n_labels)\n",
        "            labels.append(array_mask)\n",
        "        imgs = np.array(imgs)\n",
        "        labels = np.array(labels)\n",
        "        yield imgs, labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7jt3PWc0-Mo",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title   layers.py\n",
        "#layers.py\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.layers import Layer\n",
        "\n",
        "\n",
        "class MaxPoolingWithArgmax2D(Layer):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            pool_size=(2, 2),\n",
        "            strides=(2, 2),\n",
        "            padding='same',\n",
        "            **kwargs):\n",
        "        super(MaxPoolingWithArgmax2D, self).__init__(**kwargs)\n",
        "        self.padding = padding\n",
        "        self.pool_size = pool_size\n",
        "        self.strides = strides\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        padding = self.padding\n",
        "        pool_size = self.pool_size\n",
        "        strides = self.strides\n",
        "        if K.backend() == 'tensorflow':\n",
        "            ksize = [1, pool_size[0], pool_size[1], 1]\n",
        "            padding = padding.upper()\n",
        "            strides = [1, strides[0], strides[1], 1]\n",
        "            output, argmax = tf.nn.max_pool_with_argmax(\n",
        "                    inputs,\n",
        "                    ksize=ksize,\n",
        "                    strides=strides,\n",
        "                    padding=padding)\n",
        "        else:\n",
        "            errmsg = '{} backend is not supported for layer {}'.format(\n",
        "                    K.backend(), type(self).__name__)\n",
        "            raise NotImplementedError(errmsg)\n",
        "        argmax = K.cast(argmax, K.floatx())\n",
        "        return [output, argmax]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        ratio = (1, 2, 2, 1)\n",
        "        output_shape = [\n",
        "                dim//ratio[idx]\n",
        "                if dim is not None else None\n",
        "                for idx, dim in enumerate(input_shape)]\n",
        "        output_shape = tuple(output_shape)\n",
        "        return [output_shape, output_shape]\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return 2 * [None]\n",
        "\n",
        "\n",
        "class MaxUnpooling2D(Layer):\n",
        "    def __init__(self, size=(2, 2), **kwargs):\n",
        "        super(MaxUnpooling2D, self).__init__(**kwargs)\n",
        "        self.size = size\n",
        "\n",
        "    def call(self, inputs, output_shape=None):\n",
        "        updates, mask = inputs[0], inputs[1]\n",
        "        with tf.variable_scope(self.name):\n",
        "            mask = K.cast(mask, 'int32')\n",
        "            input_shape = tf.shape(updates, out_type='int32')\n",
        "            #  calculation new shape\n",
        "            if output_shape is None:\n",
        "                output_shape = (\n",
        "                        input_shape[0],\n",
        "                        input_shape[1]*self.size[0],\n",
        "                        input_shape[2]*self.size[1],\n",
        "                        input_shape[3])\n",
        "            self.output_shape1 = output_shape\n",
        "\n",
        "            # calculation indices for batch, height, width and feature maps\n",
        "            one_like_mask = K.ones_like(mask, dtype='int32')\n",
        "            batch_shape = K.concatenate(\n",
        "                    [[input_shape[0]], [1], [1], [1]],\n",
        "                    axis=0)\n",
        "            batch_range = K.reshape(\n",
        "                    tf.range(output_shape[0], dtype='int32'),\n",
        "                    shape=batch_shape)\n",
        "            b = one_like_mask * batch_range\n",
        "            y = mask // (output_shape[2] * output_shape[3])\n",
        "            x = (mask // output_shape[3]) % output_shape[2]\n",
        "            feature_range = tf.range(output_shape[3], dtype='int32')\n",
        "            f = one_like_mask * feature_range\n",
        "\n",
        "            # transpose indices & reshape update values to one dimension\n",
        "            updates_size = tf.size(updates)\n",
        "            indices = K.transpose(K.reshape(\n",
        "                K.stack([b, y, x, f]),\n",
        "                [4, updates_size]))\n",
        "            values = K.reshape(updates, [updates_size])\n",
        "            ret = tf.scatter_nd(indices, values, output_shape)\n",
        "            return ret\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        mask_shape = input_shape[1]\n",
        "        return (\n",
        "                mask_shape[0],\n",
        "                mask_shape[1]*self.size[0],\n",
        "                mask_shape[2]*self.size[1],\n",
        "                mask_shape[3]\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASwLlTK91KbS",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title model.py\n",
        "\n",
        "#model.py\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Activation, Reshape\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "#from layers import MaxPoolingWithArgmax2D, MaxUnpooling2D\n",
        "\n",
        "\n",
        "def segnet(\n",
        "        input_shape,\n",
        "        n_labels,\n",
        "        kernel=3,\n",
        "        pool_size=(2, 2),\n",
        "        output_mode=\"softmax\"):\n",
        "    # encoder\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    conv_1 = Convolution2D(64, (kernel, kernel), padding=\"same\")(inputs)\n",
        "    conv_1 = BatchNormalization()(conv_1)\n",
        "    conv_1 = Activation(\"relu\")(conv_1)\n",
        "    conv_2 = Convolution2D(64, (kernel, kernel), padding=\"same\")(conv_1)\n",
        "    conv_2 = BatchNormalization()(conv_2)\n",
        "    conv_2 = Activation(\"relu\")(conv_2)\n",
        "\n",
        "    pool_1, mask_1 = MaxPoolingWithArgmax2D(pool_size)(conv_2)\n",
        "\n",
        "    conv_3 = Convolution2D(128, (kernel, kernel), padding=\"same\")(pool_1)\n",
        "    conv_3 = BatchNormalization()(conv_3)\n",
        "    conv_3 = Activation(\"relu\")(conv_3)\n",
        "    conv_4 = Convolution2D(128, (kernel, kernel), padding=\"same\")(conv_3)\n",
        "    conv_4 = BatchNormalization()(conv_4)\n",
        "    conv_4 = Activation(\"relu\")(conv_4)\n",
        "\n",
        "    pool_2, mask_2 = MaxPoolingWithArgmax2D(pool_size)(conv_4)\n",
        "\n",
        "    conv_5 = Convolution2D(256, (kernel, kernel), padding=\"same\")(pool_2)\n",
        "    conv_5 = BatchNormalization()(conv_5)\n",
        "    conv_5 = Activation(\"relu\")(conv_5)\n",
        "    conv_6 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_5)\n",
        "    conv_6 = BatchNormalization()(conv_6)\n",
        "    conv_6 = Activation(\"relu\")(conv_6)\n",
        "    conv_7 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_6)\n",
        "    conv_7 = BatchNormalization()(conv_7)\n",
        "    conv_7 = Activation(\"relu\")(conv_7)\n",
        "\n",
        "    pool_3, mask_3 = MaxPoolingWithArgmax2D(pool_size)(conv_7)\n",
        "\n",
        "    conv_8 = Convolution2D(512, (kernel, kernel), padding=\"same\")(pool_3)\n",
        "    conv_8 = BatchNormalization()(conv_8)\n",
        "    conv_8 = Activation(\"relu\")(conv_8)\n",
        "    conv_9 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_8)\n",
        "    conv_9 = BatchNormalization()(conv_9)\n",
        "    conv_9 = Activation(\"relu\")(conv_9)\n",
        "    conv_10 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_9)\n",
        "    conv_10 = BatchNormalization()(conv_10)\n",
        "    conv_10 = Activation(\"relu\")(conv_10)\n",
        "\n",
        "    pool_4, mask_4 = MaxPoolingWithArgmax2D(pool_size)(conv_10)\n",
        "\n",
        "    conv_11 = Convolution2D(512, (kernel, kernel), padding=\"same\")(pool_4)\n",
        "    conv_11 = BatchNormalization()(conv_11)\n",
        "    conv_11 = Activation(\"relu\")(conv_11)\n",
        "    conv_12 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_11)\n",
        "    conv_12 = BatchNormalization()(conv_12)\n",
        "    conv_12 = Activation(\"relu\")(conv_12)\n",
        "    conv_13 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_12)\n",
        "    conv_13 = BatchNormalization()(conv_13)\n",
        "    conv_13 = Activation(\"relu\")(conv_13)\n",
        "\n",
        "    pool_5, mask_5 = MaxPoolingWithArgmax2D(pool_size)(conv_13)\n",
        "    print(\"Build enceder done..\")\n",
        "\n",
        "    # decoder\n",
        "\n",
        "    unpool_1 = MaxUnpooling2D(pool_size)([pool_5, mask_5])\n",
        "\n",
        "    conv_14 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_1)\n",
        "    conv_14 = BatchNormalization()(conv_14)\n",
        "    conv_14 = Activation(\"relu\")(conv_14)\n",
        "    conv_15 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_14)\n",
        "    conv_15 = BatchNormalization()(conv_15)\n",
        "    conv_15 = Activation(\"relu\")(conv_15)\n",
        "    conv_16 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_15)\n",
        "    conv_16 = BatchNormalization()(conv_16)\n",
        "    conv_16 = Activation(\"relu\")(conv_16)\n",
        "\n",
        "    unpool_2 = MaxUnpooling2D(pool_size)([conv_16, mask_4])\n",
        "\n",
        "    conv_17 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_2)\n",
        "    conv_17 = BatchNormalization()(conv_17)\n",
        "    conv_17 = Activation(\"relu\")(conv_17)\n",
        "    conv_18 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_17)\n",
        "    conv_18 = BatchNormalization()(conv_18)\n",
        "    conv_18 = Activation(\"relu\")(conv_18)\n",
        "    conv_19 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_18)\n",
        "    conv_19 = BatchNormalization()(conv_19)\n",
        "    conv_19 = Activation(\"relu\")(conv_19)\n",
        "\n",
        "    unpool_3 = MaxUnpooling2D(pool_size)([conv_19, mask_3])\n",
        "\n",
        "    conv_20 = Convolution2D(256, (kernel, kernel), padding=\"same\")(unpool_3)\n",
        "    conv_20 = BatchNormalization()(conv_20)\n",
        "    conv_20 = Activation(\"relu\")(conv_20)\n",
        "    conv_21 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_20)\n",
        "    conv_21 = BatchNormalization()(conv_21)\n",
        "    conv_21 = Activation(\"relu\")(conv_21)\n",
        "    conv_22 = Convolution2D(128, (kernel, kernel), padding=\"same\")(conv_21)\n",
        "    conv_22 = BatchNormalization()(conv_22)\n",
        "    conv_22 = Activation(\"relu\")(conv_22)\n",
        "\n",
        "    unpool_4 = MaxUnpooling2D(pool_size)([conv_22, mask_2])\n",
        "\n",
        "    conv_23 = Convolution2D(128, (kernel, kernel), padding=\"same\")(unpool_4)\n",
        "    conv_23 = BatchNormalization()(conv_23)\n",
        "    conv_23 = Activation(\"relu\")(conv_23)\n",
        "    conv_24 = Convolution2D(64, (kernel, kernel), padding=\"same\")(conv_23)\n",
        "    conv_24 = BatchNormalization()(conv_24)\n",
        "    conv_24 = Activation(\"relu\")(conv_24)\n",
        "\n",
        "    unpool_5 = MaxUnpooling2D(pool_size)([conv_24, mask_1])\n",
        "\n",
        "    conv_25 = Convolution2D(64, (kernel, kernel), padding=\"same\")(unpool_5)\n",
        "    conv_25 = BatchNormalization()(conv_25)\n",
        "    conv_25 = Activation(\"relu\")(conv_25)\n",
        "\n",
        "    conv_26 = Convolution2D(n_labels, (1, 1), padding=\"valid\")(conv_25)\n",
        "    conv_26 = BatchNormalization()(conv_26)\n",
        "    conv_26 = Reshape(\n",
        "            (input_shape[0]*input_shape[1], n_labels),\n",
        "            input_shape=(input_shape[0], input_shape[1], n_labels))(conv_26)\n",
        "\n",
        "    outputs = Activation(output_mode)(conv_26)\n",
        "    print(\"Build decoder done..\")\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name=\"SegNet\")\n",
        "\n",
        "    return model\n",
        "model = segnet((512, 512 ,3), 2,\n",
        "            3, (2, 2), \"softmax\")\n",
        "print(model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e82KJXIP1k7-",
        "colab_type": "code",
        "outputId": "979a7515-2729-49cc-b961-6030d3dbae55",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "#@title training.py\n",
        "\n",
        "#train.py\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "#from  model import segnet\n",
        "#from generator import data_gen_small\n",
        "\n",
        "\n",
        "def main():\n",
        "    # set the necessary list\n",
        "    train_list = pd.read_csv(\"/content/drive/My Drive/sarseg/Data/complete_dataset/crop/train_list.csv\", header=None)\n",
        "    val_list = pd.read_csv(\"/content/drive/My Drive/sarseg/Data/complete_dataset/crop/val_list.csv\", header=None)\n",
        "\n",
        "    # set the necessary directories\n",
        "    trainimg_dir = \"/content/drive/My Drive/sarseg/Data/complete_dataset/crop/images/\"\n",
        "    trainmsk_dir = \"/content/drive/My Drive/sarseg/Data/complete_dataset/crop/gt/\"\n",
        "    valimg_dir = \"/content/drive/My Drive/sarseg/Data/complete_dataset/crop/images/\"\n",
        "    valmsk_dir = \"/content/drive/My Drive/sarseg/Data/complete_dataset/crop/gt/\"\n",
        "\n",
        "    train_gen = data_gen_small(trainimg_dir, trainmsk_dir,\n",
        "            train_list, 8,\n",
        "            [512, 512], 2)\n",
        "    val_gen = data_gen_small(valimg_dir, valmsk_dir,\n",
        "            val_list, 8,\n",
        "            [512, 512], 2)\n",
        "\n",
        "    model = segnet((512, 512 ,3), 2,\n",
        "            3, (2, 2), \"softmax\")\n",
        "    print(model.summary())\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\",\n",
        "            optimizer=\"adadelta\", metrics=[\"accuracy\"])\n",
        "    model.fit_generator(train_gen, steps_per_epoch=8,\n",
        "            epochs=8, validation_data=val_gen,\n",
        "            validation_steps=8)\n",
        "\n",
        "    model.save_weights(\"/content/drive/My Drive/sarseg/\"+str(00)+\".hdf5\")\n",
        "    model.save(\"/content/drive/My Drive/sarseg/\"+str(00)+\".h5\")\n",
        "    print(\"save weight done..\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f9edee240a8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-f9edee240a8a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     model = segnet((512, 512 ,3), 2,\n\u001b[0;32m---> 29\u001b[0;31m             3, (2, 2), \"softmax\")\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-677f9d63aab3>\u001b[0m in \u001b[0;36msegnet\u001b[0;34m(input_shape, n_labels, kernel, pool_size, output_mode)\u001b[0m\n\u001b[1;32m     16\u001b[0m         output_mode=\"softmax\"):\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mconv_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvolution2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[1;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                              input_tensor=tensor)\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'input'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;34m'It looks like you are trying to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;34m'a version of multi-backend Keras that '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;34m'does not support TensorFlow 2.0. We recommend '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: It looks like you are trying to use a version of multi-backend Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04c_uDQyPaGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB6zJPjjHVea",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title   loading model\n",
        "\n",
        "import keras\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import load_model\n",
        "model_test = keras.models.load_model('/content/drive/My Drive/sarseg/Data/modelout/run0.h5',\n",
        "                                     custom_objects={'MaxPoolingWithArgmax2D': MaxPoolingWithArgmax2D, 'MaxUnpooling2D':MaxUnpooling2D})\n",
        "#model_test = segnet((512, 512 ,3), 2,\n",
        "#          3, (2, 2), \"softmax\")\n",
        "model_test.load_weights('/content/drive/My Drive/sarseg/Data/modelout/run0.hdf5')\n",
        "print('model loaded')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryj6KFidCOPx",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title   getting loaded model accuracy\n",
        "\n",
        "\n",
        "val_list = pd.read_csv(\"/content/drive/My Drive/sarseg/Data/complete_dataset/crop/val_list.csv\", header=None)\n",
        "\n",
        "valimg_dir = \"/content/drive/My Drive/sarseg/Data/complete_dataset/crop/images/\"\n",
        "valmsk_dir = \"/content/drive/My Drive/sarseg/Data/complete_dataset/crop/gt/\"\n",
        "\n",
        "val_gen = data_gen_small(valimg_dir, valmsk_dir,\n",
        "             val_list, 1,\n",
        "            [512, 512], 2)\n",
        "\n",
        "model_test.compile(loss=\"categorical_crossentropy\",\n",
        "            optimizer=\"adadelta\", metrics=[\"accuracy\"])\n",
        "score = model_test.evaluate(val_gen, steps = 1)\n",
        "print(\"%s: %.2f%%\" % (model_test.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vsb_qA0Fjjt",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title predicting output\n",
        "\n",
        "valimg_dir = \"/content/drive/My Drive/sarseg/Data/complete_dataset/crop/images/\"\n",
        "valmsk_dir = \"/content/drive/My Drive/sarseg/Data/complete_dataset/crop/gt/\"\n",
        "\n",
        "test = np.zeros([1, 512, 512, 3])\n",
        "test_image = cv2.imread(valimg_dir + 'austin100.tif')\n",
        "tested = cv2.resize(test_image, (512, 512))\n",
        "test[0] = img_to_array(tested)\n",
        "\n",
        "mask = np.zeros([1, 512, 512, 3])\n",
        "test_image = cv2.imread(valmsk_dir + 'austin100.tif')\n",
        "tested = cv2.resize(test_image, (512, 512))\n",
        "array_mask = category_label(tested[:, :, 0], (512, 512, 3), 2)\n",
        "\n",
        "predicted_out = model_test.predict(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Daq4HtxR_xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(predicted_out)\n",
        "print(array_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvY16ZUT8VPA",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title   making image arrays for matplot\n",
        "\n",
        "array_mask.shape\n",
        "mask = np.zeros([512, 512])\n",
        "for i in range(512):\n",
        "  for j in range(512):\n",
        "    mask[i][j] = array_mask[i*512+j][0];\n",
        "\n",
        "predicted = predicted_out[0]\n",
        "predicted.shape\n",
        "pred = np.zeros([512, 512])\n",
        "for i in range(512):\n",
        "  for j in range(512):\n",
        "    pred[i][j] = 1-predicted[i*512+j][0];\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQnsahgyHvhQ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title   plotting images\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(2, 2, 1)\n",
        "ax1.imshow(test[0].astype(int))\n",
        "ax2 = fig.add_subplot(2, 2, 2)\n",
        "ax2.imshow(mask, cmap= 'Greys')\n",
        "ax3 = fig.add_subplot(2, 2, 3)\n",
        "ax3.imshow(pred, cmap= 'Greys')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD69rsTR3fy-",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title   printing prediction accuracy\n",
        "\n",
        "from keras import backend as K\n",
        "total = 512*512\n",
        "intersection = 0\n",
        "for i in range(512):\n",
        "  for j in range(512):\n",
        "    if( mask[i][j] == pred[i][j] ):\n",
        "        intersection = intersection + 1 \n",
        "    \n",
        "print(intersection/total)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}